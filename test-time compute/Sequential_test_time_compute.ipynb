{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSZCkPN04-v0"
      },
      "source": [
        "## Model configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ijgsdZZ5Cyt"
      },
      "source": [
        "### Generation of Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1q8STyP4I67"
      },
      "outputs": [],
      "source": [
        "hf_token = \"HF_TOKEN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1aZXQL954wAy"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "bbbd1b426f694cd28dac4f0d9b4dcf8f",
            "5615f1f42ad24e6d8c598065f62cceb5",
            "6ddefb316c5b411c9727786d560a5fee",
            "1de9e53c03994fcdb4a9c8302262cd73",
            "7aa4c226defe4836b756d61038f56e2d",
            "293962ce6fa544f0a16fdd97cfa63225",
            "8a999a4c664947b4801c9f8e31d35a65",
            "8f38eeac4bf947ca9144b825e79e8e4b",
            "d35086708d7f4ee4999d2e5ad7a568dc",
            "2dbcabab26a34e159e7fa1822954197a",
            "710cc93ba04640b38eafd29884d25ab0",
            "e3e7c486c0cd4ad382f0234b870d50a2",
            "2709ab1088bc468994ee06469a8942d8",
            "56e601a3397844969e0f0fd01115dc43",
            "cf9a61db7a824adeb3c63d637e9d012a",
            "2364a65b6d9846e799ec5f3f0d9c7b3f",
            "c09ad3e450c14711a2cb44b6b20c7dd0",
            "d931a54f247841ae9fc8343686351483",
            "8c9dd734a1704c2b906a60a1ddb8aaf0",
            "90705f9ffae34a18a061167f65b56e85",
            "d676835dfb124a898f0ffe651a1ff615",
            "b1a5b68114b34418beab0f0bcab41b6c",
            "e13e143ae03a41f09af3ff60b20c1ff4",
            "133340aba5d0465e9c180b0e43d87b60",
            "013600442b2d400eb645f844789e9d15",
            "199783c862634f18ae92695ca0fe4112",
            "e8342560b715463dafc9a50f802e086a",
            "bbd8fecdd8e3430b8b845b1a041afc86",
            "fa1625575f52400b9c1839d297ed7885",
            "ab6e621fc05c44829238af710ae5635b",
            "5ffb48b97ec7472395691e32a7268d43",
            "fda2ce24579f41099c3bfc7c965a6460",
            "9ff95949d92b47dbb6d27b52c96cad16",
            "07d3ae44529c4b48b20f18714e50c269",
            "793e15dc8fc34ebfb04da20676f78825",
            "cfb6f2386e124a5c92fe03e09b188e0d",
            "0eaa1ab92fc74953a6b642e17347825f",
            "c81c1472f8be4547b0271c2e867e3636",
            "6ee88d87bc7e438e8e22d9ffba45afbd",
            "64da57205f994e96b0588434a306c14b",
            "c475483828134b0ab0e9b97be990c9e3",
            "26da12bd16de48b789118476a1539ca8",
            "298517ce0b49432a93d8dfbe1ef2e44b",
            "38d9d8a194ea4af9ac2da8fb3bef37e2",
            "1b1f17b08e5547f9a6d8c08d7a1df92a",
            "48b99a58b9f34ab1b0bdda9a16376ec3",
            "e7eee2f839d34108bb4c240dccd3507d",
            "99e67e759e8449dd94ae9836186d7bad",
            "6ab6512db4d548c9a4f6af3ca235f3a2",
            "aa00d66c6937496ab501f321fe2e5489",
            "36cac0c27cf648d1b9dd47e32dcacff4",
            "845d8688338b4e1aa8c028733b8be53d",
            "c4709734118047c2aa3ace0fafc4000c",
            "f451409f94624574b313cf1ffa7e0a6e",
            "dce5a472404f4affb4ff887d1562205d",
            "5d6e072bc0384769b0ab484831ab4065",
            "49fa9ba8ffd54238bf391dbef026883e",
            "ec541856be544352a71955be701be609",
            "f80d007c85b14d5697c3119424f0f0bb",
            "ec2fce76f9ab4289a08c7d630ad7c0c3",
            "d3cb2247816046cdabd7b3b453ea70ed",
            "ccd94f9ae7e34a32bb7c95e584b719ea",
            "eb5ee21bba1548beabbec658b13478af",
            "dce4b122ce734c96956ca5cf95d04345",
            "bf0dbace33da4415bb2ac88c34cd6074",
            "78619268e56e4dffb7f10f577ae734a2"
          ]
        },
        "id": "92xj53nF5c5K",
        "outputId": "edaddeb0-dd4a-434c-c1ed-f355df1d2fd3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3jFy913eUupZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC5N5D9T5MAo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def generate_next_steps(context=\"hey\", M=5, temperature=0.8, max_new_tokens=200):\n",
        "    \"\"\" Generate M next steps based on the provided context using the model.\n",
        "    Args:\n",
        "    - context (str): The input context to generate steps from.\n",
        "    - M (int): Number of steps to generate (default: 5).\n",
        "    - temperature (float): Sampling temperature (default: 0.8).\n",
        "    - max_new_tokens (int): Maximum number of new tokens to generate (default: 200).\n",
        "    Returns:\n",
        "    - List of generated texts.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(context, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        num_return_sequences=M,\n",
        "        max_new_tokens=max_new_tokens\n",
        "    )\n",
        "\n",
        "    generated_texts = []\n",
        "\n",
        "    for i in range(M):\n",
        "        generated = tokenizer.decode(outputs[i][inputs['input_ids'].shape[-1]:], skip_special_tokens=False)\n",
        "        generated_texts.append(generated.strip())\n",
        "\n",
        "    return generated_texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzOwz-pkJ9fM"
      },
      "outputs": [],
      "source": [
        "def slice_str(text):\n",
        "  \"\"\"Parses the steps from the text.\"\"\"\n",
        "\n",
        "  steps_list = text.split(\"## Step\")\n",
        "\n",
        "  if len(steps_list) == 0:\n",
        "    return text\n",
        "\n",
        "  if len(steps_list[0]) < 2:\n",
        "    return steps_list[1]\n",
        "  else:\n",
        "    return steps_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg50pHiQD3G_"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9VdEBQsq3AWu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\"hf://datasets/meta-llama/Llama-3.2-1B-Instruct-evals/Llama-3.2-1B-Instruct-evals/Details_math_2024-09-23T17-23-17.197184.parquet.gzip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WlW0Y7EO3CzU"
      },
      "outputs": [],
      "source": [
        "sample_prompt = df['input_final_prompts'][0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGz35c3g8hrO",
        "outputId": "4dbe8b96-a2fe-418a-f021-2a41e44fff6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "ans = generate_next_steps(temperature=0.8, context=sample_prompt, max_new_tokens=204)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHTnYP3SCu-V",
        "outputId": "95b177c8-b579-4125-a7c1-32c552a58491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1: Use De Moivre's Theorem to find the expression for $(\\cos12^\\circ+i\\sin12^\\circ+\\cos48^\\circ+i\\sin48^\\circ)^6$\n",
            "De Moivre's Theorem states that for any real number $x$ and integer $n$, $(\\cos x + i\\sin x)^n = \\cos(nx) + i\\sin(nx)$. Therefore, we can rewrite the expression as $[(\\cos 12^\\circ + i\\sin 12^\\circ) + (\\cos 48^\\circ + i\\sin 48^\\circ)]^6$.\n",
            "\n",
            "\n",
            "----------------------\n",
            " 1: Apply De Moivre's Theorem\n",
            "To find the imaginary part of the expression, we can first express the given expression in polar form using De Moivre's Theorem, which states that $(\\cos\\theta+i\\sin\\theta)^n = \\cos(n\\theta)+i\\sin(n\\theta)$. In this case, we have $(\\cos12^\\circ+i\\sin12^\\circ+\\cos48^\\circ+i\\sin48^\\circ)^6$. We can express the first two terms as $(\\cos12^\\circ+i\\sin12^\\circ+\\cos48^\\circ+i\\sin48^\\circ) = (\\cos(60^\\circ)+i\\sin(60^\\circ))+(\\cos(96^\\circ)+i\\sin(96^\\circ))$.\n",
            "\n",
            "\n",
            "----------------------\n",
            " 1: Convert the complex number to polar form\n",
            "We first express the given complex number in polar form: $re^{i\\theta} = r(\\cos\\theta + i\\sin\\theta)$, where $r$ is the magnitude and $\\theta$ is the argument. The magnitude is $\\sqrt{(\\cos12^\\circ+i\\sin12^\\circ)^2+(\\cos48^\\circ+i\\sin48^\\circ)^2}$.\n",
            "\n",
            "\n",
            "----------------------\n",
            " 1: Apply De Moivre's Theorem\n",
            "We can express the given expression in the form $r(\\cos\\theta+i\\sin\\theta)$, where $r=\\sqrt{(\\cos12^\\circ+i\\sin12^\\circ)^2+(\\cos48^\\circ+i\\sin48^\\circ)^2}$ and $\\theta=\\arctan\\left(\\frac{\\sin48^\\circ}{\\cos48^\\circ}\\right)$.\n",
            "\n",
            "\n",
            "----------------------\n",
            " 1: Apply De Moivre's Theorem\n",
            "To find the imaginary part of the given expression, we first need to apply De Moivre's Theorem, which states that $(\\cos\\theta+i\\sin\\theta)^n = \\cos(n\\theta)+i\\sin(n\\theta)$.\n",
            "\n",
            "\n",
            "----------------------\n"
          ]
        }
      ],
      "source": [
        "for i in enumerate(ans):\n",
        "  strings = slice_str(i[1])\n",
        "  print(strings)\n",
        "  print(\"----------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga_RtXV22BZq"
      },
      "source": [
        "### Process Reward Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WXkr9OSk_Rti"
      },
      "outputs": [],
      "source": [
        "next_step = slice_str(ans[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "273db950214b498a8c4754d275f37020",
            "f235f006aae5400fb71a9c7156207f47",
            "6609b66eaa2f46f4bc45e5595edbd5c6",
            "0801e009db5e46a0870d538c97be9739",
            "769bf49b09f04be5928d2f19946440c0",
            "46259a4fa8ec4dba8c97101ac897b97a",
            "21652bb18be14df3ba6137dba8108407",
            "c411baa649e245ba8676489fe3894ea3",
            "6e01316cf2414689a896d4929dfc7ff9",
            "64d9a33ac73443389f3e00191ef6f1c6",
            "28d7c79bc4c74d7ab52876797fa655d3",
            "168536fd051442d2adb4689001200cdd",
            "5f87e019f28c4aa28d62d75d6719034b",
            "52c83ec7e94d464b8896c341ff831831",
            "43928f9e3e98437fa14a9cb3ae835c3c",
            "639d86582dbc4913941f333d020953d1",
            "9086834d1ccf4465bc6cf1ee7dc553d2",
            "385079cf26494581904116fa49e5f922",
            "edabf920150b47db942eac6f80f13218",
            "75b31142c80046b8b2bf7d17b0d3570a",
            "075c0c0dc3b3435ab0b48599fa8ff7cd",
            "426bd5d76b6e46299f64a7b48f5b6f99",
            "e60f71163cd64a61b4258f7864c9a7c9",
            "93165948435e47ad9edaf9771a29b149",
            "fc2d0cc7eadf4cedbdb75cc8f413ab5f",
            "690a947ccecd4c1dacd20e86ad2fc46f",
            "7a1935f775ec41d0acb2cab96cdfb208",
            "78dfea80d2e24ba5a78efa0be9fd8b8b",
            "0d425c66d93447ed8c1f73eded10848c",
            "1cbe13d3ad6e4e7c80980ee0dd704ae6",
            "625e1edb1f8f46d6877df4c0ebdae27e",
            "de1da69150c54de9b2c5ca2698322a6f",
            "309656c3daaa48cf894bb5893a8409fd",
            "78f70fed2c6a456a9a38277dbddc54c4",
            "3951c48d7191498db5597294b58a6105",
            "0c0476b497694ff78f9a604d69c24a1f",
            "952d7ad05de4453eada13d890c4eb4cf",
            "463fb0dc78294548a405b5153d4d5bf9",
            "c595fcbb742d4cdf84a536a69a2f442b",
            "1b89e41f185a45be8d19fc654c2691ea",
            "e52900dec7b3406ab10bb8a32deb304d",
            "f810414d192444dc81859aad4452350b",
            "7bc4f74b90394a029686252a223f0840",
            "a1b1ea9ef78146d6b1f2c2d7c003a82b",
            "c84ae1db2e384742a59e83cbee53068e",
            "b4c8b751b8ea4592bb5f6c2d04fbee20",
            "da72b31a28da44b186d7d388f3b019e9",
            "b3ee540ce7d141a586242f5d7e3e881c",
            "b93891f7e545443382ff778ec315ece5",
            "ecb76e5b682c40c2bf9ea0c64d2a9014",
            "1cefa6cd60a849ff8323c5f6ca879c1d",
            "4ab215c829714c6095c77f20de578bda",
            "bd288f2d7cbd4aac8bee1b20e0c974fd",
            "e186f4f0b615458ca1764385549f816e",
            "e2e0dd0732cc4b338408e9239f604d15",
            "4e9d4e870a6140dcb9c3c6c62c254ca4",
            "0a20b7483a334573873ec257ea5074be",
            "ce472e4e9d31498ba436f0f7e5a3e418",
            "d837f2b9c6c54705a83260bddabd62d8",
            "a20b1f260e1d4d9d87f5748d2cae2c4d",
            "0691087721fc4a2a866c0287d9123517",
            "e09bcebfe3164c0884e9f4a3c42f23a4",
            "93cd042735484ae7a9d27c4e7dc3f88c",
            "0d7af379dca34f7ea5d696bdff2bba92",
            "018cbceb84ff472db9fd01840b7d2141",
            "17512378d3da4273b5b19c4805611c88",
            "26a72eae45864deab62e78d41d8ee67f",
            "9d1e8e6ff38f4359bf1a8583276f5590",
            "f240663b3c90427991ff4297b71b80e9",
            "fb7c917c534846eb8b50fa5acd41c1ba",
            "8448a50ce2524449b8edd49470665a18",
            "34344783d17846caadb1dc07a9689e28",
            "677992f988c0405590f96017b3a37ecb",
            "71db8186c2194ffc9fcf925ebd0b37d6",
            "e0a761fd2ab3496f9711e08b326c3883",
            "8f8b7441871b4862b2c41a6abf597f7d",
            "b1bf474193ac4f848fe4b4a88b04bb18"
          ]
        },
        "id": "DRIE5KrFLI4w",
        "outputId": "e2e52a2c-92d4-43cb-be92-ed0e3c012068"
      },
      "outputs": [],
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "prm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "prm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aChdjeG-XSmb"
      },
      "outputs": [],
      "source": [
        "def score_intermediary_step(steps, positive_token=\"+\", negative_token=\"-\"):\n",
        "    \"\"\"\n",
        "    Scores an intermediary step in a reasoning process using the model as a PRM.\n",
        "\n",
        "    Args:\n",
        "    - steps (list of str): List of steps in the reasoning process, where:\n",
        "        - steps[0] is the task description,\n",
        "        - steps[1:-1] are previous intermediary steps,\n",
        "        - steps[-1] is the current intermediary step to evaluate.\n",
        "    - positive_token (str): Token for positive reward (default: '+').\n",
        "    - negative_token (str): Token for negative reward (default: '-')\u2014used for comparison.\n",
        "\n",
        "    Returns:\n",
        "    - float: Normalized score between 0 and 1, where higher values indicate better steps.\n",
        "    \"\"\"\n",
        "\n",
        "    task_description = steps[0]\n",
        "    curr_step = steps[-1]\n",
        "    prev_steps = steps[1:-1]\n",
        "\n",
        "    prev_steps_str = \"\\n\\n## Step\"+\"\\n\\n## Step\".join(prev_steps)\n",
        "\n",
        "    prompt = f\"\"\"<|system|>\n",
        "      You are a Process Reward Model. Evaluate the following current intermediary step in the reasoning process. Output only a single token: '+' if the step is correct and helpful, or '-' if it's incorrect or unhelpful.</s>\n",
        "      <|user|>\n",
        "      Task: {task_description}\n",
        "      Previous Intermediary Steps:{prev_steps_str}\n",
        "      Current Intermediary Step: {curr_step}</s>\n",
        "      <|assistant|>\n",
        "      Verdict:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    pos_id = tokenizer.convert_tokens_to_ids(positive_token)\n",
        "    neg_id = tokenizer.convert_tokens_to_ids(negative_token)\n",
        "\n",
        "    if pos_id == tokenizer.unk_token_id or neg_id == tokenizer.unk_token_id:\n",
        "        raise ValueError(f\"Tokens '{positive_token}' or '{negative_token}' not in vocab. Try different tokens (e.g., 'Yes'/'No').\")\n",
        "\n",
        "    # Compute logprobs\n",
        "    logprobs = torch.log_softmax(logits, dim=-1)\n",
        "    pos_logprob = logprobs[0, pos_id].item()\n",
        "    neg_logprob = logprobs[0, neg_id].item()\n",
        "\n",
        "    import math\n",
        "    score_diff = pos_logprob - neg_logprob\n",
        "    normalized_score = 1 / (1 + math.exp(-score_diff))\n",
        "\n",
        "    verdict = positive_token if pos_logprob > neg_logprob else negative_token\n",
        "\n",
        "    return normalized_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-XWZwkQzpK4",
        "outputId": "30aeb08d-1b91-44fb-ccb4-f71208e755b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.985155100397563\n",
            "----------------------\n",
            "0.9877642469621631\n",
            "----------------------\n",
            "0.9854197920797677\n",
            "----------------------\n",
            "0.9864782356042919\n",
            "----------------------\n",
            "0.981159061791835\n",
            "----------------------\n"
          ]
        }
      ],
      "source": [
        "for step in enumerate(ans):\n",
        "  result = score_intermediary_step(sample_prompt, step[1])\n",
        "  print(result)\n",
        "  print(\"----------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-L1X1RADSBW"
      },
      "source": [
        "### Generation loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Gyo1w0IPFQaF"
      },
      "outputs": [],
      "source": [
        "eos_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "eos_token = eos_tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "59idpytPFkFB",
        "outputId": "6e17c752-c956-4c61-f7bb-c503c7415cfe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|eot_id|>'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP8jfFtLg-tX"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, text, score=None, parent=None, depth=0, is_terminal=False):\n",
        "        self.text = text\n",
        "        self.score = score\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        self.depth = depth\n",
        "        self.is_terminal = False\n",
        "\n",
        "    def add_children(self, texts, eos_token=\"<|eot_id|>\"):\n",
        "        self.children = [Node(t, parent=self, depth=self.depth + 1, is_terminal=(eos_token in t)) for t in texts]\n",
        "        return self.children\n",
        "\n",
        "    def path(self):\n",
        "        # Return full path from root to this node\n",
        "        node, path = self, []\n",
        "        while node:\n",
        "            path.append(node.text)\n",
        "            node = node.parent\n",
        "        return path[::-1]\n",
        "\n",
        "class TreeSampler:\n",
        "    \"\"\"A class to manage a tree of nodes, allowing expansion and scoring of nodes.\n",
        "    It starts with a root node and expands it by generating candidates and scoring them.\n",
        "    The leaves of the tree are the active nodes that can be expanded further.\n",
        "    Each node can be terminal or non-terminal, and the tree can be pruned to keep only the top K nodes based on their scores, somewhat similar to beam search.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_text=\"\", eos_token=\"<|eot_id|>\"):\n",
        "        self.root = Node(root_text)\n",
        "        self.leaves = [self.root]\n",
        "        self.eos_token = eos_token\n",
        "\n",
        "    def expand_and_score(self, N, generator_fn, scorer_fn):\n",
        "        \"\"\"\n",
        "        -> generator_fn - Expects a function that returns a list of candidates, and takes node.path(), N as input\n",
        "        -> scorer_fn - Expects a function that returns a score, and takes candidate.path() as input\n",
        "        \"\"\"\n",
        "        new_leaves = []\n",
        "        for node in self.leaves:\n",
        "            if node.is_terminal:\n",
        "                new_leaves.append(node)\n",
        "                continue\n",
        "            candidates = generator_fn(node.path(), N)\n",
        "            children = node.add_children(candidates, eos_token=self.eos_token)\n",
        "            for child in children:\n",
        "                child.score = scorer_fn(child.path())\n",
        "            new_leaves.extend(children)\n",
        "        self.leaves = new_leaves\n",
        "\n",
        "    def select_top_k(self, K):\n",
        "        self.leaves = sorted(self.leaves, key=lambda x: x.score, reverse=True)[:K]\n",
        "\n",
        "    def get_top_paths(self, K=1):\n",
        "        top_nodes = sorted(self.leaves, key=lambda x: x.score, reverse=True)[:K]\n",
        "        return [node.path() for node in top_nodes]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a743KDX5kZcq"
      },
      "outputs": [],
      "source": [
        "def gen(path, N):\n",
        "  responses = []\n",
        "  if len(path) < 2:\n",
        "    path.append(\" \") # To help the model generate the required format\n",
        "    \n",
        "  response = generate_next_steps(temperature=0.8, context=\"\\n\\n## Step\".join(path), max_new_tokens=204)\n",
        "  for step in enumerate(response):\n",
        "    curr_step = slice_str(step[1])\n",
        "    responses.append(curr_step)\n",
        "  return responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry2ZoDJlh-Ay",
        "outputId": "01591a2c-8e15-47c2-f1c1-b40830e110b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "solution_tree = TreeSampler(root_text=sample_prompt, eos_token=eos_token)\n",
        "\n",
        "max_depth = 5\n",
        "\n",
        "for _ in range(max_depth):\n",
        "    solution_tree.expand_and_score(\n",
        "        N=4,\n",
        "        generator_fn=gen,\n",
        "        scorer_fn=score_intermediary_step\n",
        "    )\n",
        "    solution_tree.select_top_k(K=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo9g3oyvbKYW",
        "outputId": "b2c85e57-65b9-4872-b529-1df33a7baf07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3: Find the imaginary part\n",
            "Now we need to find the imaginary part of $(\\cos60^\\circ+i\\sin60^\\circ)$. The imaginary part of a complex number in polar form is given by $i\\sin\\theta$, where $\\theta$ is the angle in radians. Here, $\\theta = 60^\\circ = \\frac{\\pi}{3}$ radians. Therefore, the imaginary part is $64i\\sin\\frac{\\pi}{3} = 64i\\cdot \\frac{\\sqrt{3}}{2} = 32i\\sqrt{3}$.\n",
            "\n",
            "The final answer is: $\\boxed{32i\\sqrt{3}}$. I hope it is correct.<|eot_id|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>\n",
            "----------------------\n",
            "3: Find the imaginary part\n",
            "Now we need to find the imaginary part of $(\\cos60^\\circ+i\\sin60^\\circ)$. The imaginary part of a complex number in polar form is given by $i\\sin\\theta$, where $\\theta$ is the angle in radians. Here, $\\theta = 60^\\circ = \\frac{\\pi}{3}$ radians. Therefore, the imaginary part is $64i\\sin\\frac{\\pi}{3} = 64i\\cdot \\frac{\\sqrt{3}}{2} = 32i\\sqrt{3}$.\n",
            "\n",
            "The final answer is: $\\boxed{32i\\sqrt{3}}$. I hope it is correct.<|eot_id|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>\n",
            "----------------------\n"
          ]
        }
      ],
      "source": [
        "for path in enumerate(solution_tree.get_top_paths(K=2)):\n",
        "    print(path[1][-3])\n",
        "    print(\"----------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCeWi_qSg0zM"
      },
      "source": [
        "### Further Work\n",
        "The EOT token is not working. The model scores and generates even after a eot token is generated"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}